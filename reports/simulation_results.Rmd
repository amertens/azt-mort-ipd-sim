---
title: "Simulation results"
author: "Andrew Mertens"
date: "2024-11-18"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(here)
library(knitr)

load(here("results/meta_performance.rdata"))

clean_tab <- function(tab){
  tab$abs_bias <- tab$abs_bias*1000
  tab$mean_variance <- tab$mean_variance*1000
  
  tab$abs_bias <- round(tab$abs_bias, 2)
  tab$mean_variance <- round(tab$mean_variance, 2)
  tab$bias_se_ratio <- round(tab$bias_se_ratio, 2)
  tab$O_coverage <- round(tab$O_coverage, 1)
  tab$power <- round(tab$power, 1)
  return(tab)
}

tabRD_RE <- clean_tab(tabRD_RE)
tabRR_RE <- clean_tab(tabRR_RE)
tabRR_RE <- clean_tab(res_RE_summarized)
       

```

#Write a paragraph here about the setup of the simulation in this repo:



## Simulation tables

Note the bias and variance columns are multiplied by 1000 for readability

```{r}

knitr::kable(tabRD_RE)
knitr::kable(tabRR_RE)
knitr::kable(res_RE_summarized)


```

## Plot results

**Figure 1.** Performance metrics for the cumulative incidence difference across different subgroups pooled via random effects meta-analyses. The mean absolute bias,  mean variance, bias to standard error ratio, power, oracle coverage, and coverage are shown for each subgroup. The oracle coverage is the proportion of confidence intervals that contain the true value. The coverage is the proportion of confidence intervals that contain the true value where the confidence interval is constructed from the variance of the point estimates across the simulation iteractions. 

```{r, echo=FALSE}
ggplot(resRR_long ,
         aes(x=level, y=value, color=metric, shape=metric  )) +
  coord_flip() +
  geom_point(position = position_dodge(0.5)) +
  facet_wrap(~performance_metric, scales="free") +
  theme_minimal() + theme(legend.position = c(0.8,0.25)) + 
  guides(color=guide_legend(title="Estimator"), shape=guide_legend(title="Estimator"))

```


